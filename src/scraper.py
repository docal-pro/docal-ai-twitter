from datetime import datetime
import json
import os
import sys
import asyncio
import psycopg2
from pathlib import Path
from dotenv import load_dotenv
from utils.utils import connect_to_database, add_to_database, add_to_score, add_to_schedule

# Load environment variables
load_dotenv()


async def get_tweets(tweet_ids: list[str], flag: str = "none"):
    """Get tweet data using agent-twitter-client"""
    print(f"üîé Searching for tweets...")
    try:
        process = await asyncio.create_subprocess_exec(
            "node",
            "agent/scraper.js",
            ",".join(tweet_ids),
            flag,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        print(f"‚ÑπÔ∏è  Waiting for tweets...")
        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=30)

        if stderr:
            stderr_text = stderr.decode()
            print(f"‚ùå Error: {stderr_text}")
            schedule_data = {
                "caller": caller,
                "transaction": transaction,
                "username": "",
                "tweet_ids": tweet_ids,
                "contexts": ctxs,
            }
            print(schedule_data)
            add_to_schedule(schedule_data)
            return []

        # Decode stdout
        decoded_data = stdout.decode("utf-8")

        # Separate logs and JSON output
        lines = decoded_data.split("\n")
        logs, json_lines = [], []

        for line in lines:
            stripped = line.strip()
            if stripped.startswith("["):  # JSON array of tweets
                json_lines.append(stripped)
            elif json_lines:  # Handle multi-line JSON
                json_lines.append(stripped)
            else:
                logs.append(stripped)

        # Print logs
        for log in logs:
            print(log)

        # Parse JSON
        json_string = "".join(json_lines) if json_lines else "[]"

        try:
            tweets_data = json.loads(json_string)
        except json.JSONDecodeError as e:
            print(f"\n‚ùå Error parsing JSON: {e}")
            return []

        if not tweets_data:
            print(f"‚ùå No tweets found")
            return []
        
        # Add to schedule
        schedule_data = {
            "caller": caller,
            "transaction": transaction,
            "username": "",
            "tweet_ids": [],
            "contexts": ctxs,
        }
        print(schedule_data)
        add_to_schedule(schedule_data)
        return tweets_data

    except Exception as e:
        print(f"‚ùå Error fetching tweets: {e}")
        return []
    

def check_existing_tweets(tweet_ids: list[str]) -> tuple[bool, str, list]:
    """
    Check if tweets already exist in the database
    Returns: (exists: bool, username: str if exists else None, existing_tweets: list)
    """
    print(f"üîé Checking for tweets...")
    
    # Query the database for the tweet IDs
    try:
        connection, cursor = connect_to_database()

        # Query for tweets
        placeholders = ','.join(['%s'] * len(tweet_ids))
        cursor.execute(
            f"SELECT tweet_id, username, tweet FROM twitter WHERE tweet_id IN ({placeholders})",
            tuple(tweet_ids)
        )
        results = cursor.fetchall()

        if results:
            matched_tweet_ids = [row[0] for row in results]
            matched_usernames = [row[1] for row in results]
            matched_tweets = [row[2] for row in results]
            return True, matched_tweet_ids, matched_usernames, matched_tweets
            
        cursor.close()
        connection.close()
    except Exception as e:
        print(f"‚ùå Error querying database: {e}")
        
    return False, [], [], []


async def main():
    if len(sys.argv) != 6:
        print("‚ùå Incorrect number of arguments")
        print("‚ÑπÔ∏è  Usage: python scraper.py <tweet_ids> <flag> <contexts> <caller> <transaction>")
        sys.exit(0)

    # Parse comma-separated tweet IDs
    tweet_ids = sys.argv[1].split(',')
    flag = sys.argv[2]
    ctxs = sys.argv[3].split(',')
    caller = sys.argv[4]
    transaction = sys.argv[5]
    
    # First check if tweets already exist
    status, existing_tweet_ids, existing_usernames, existing_tweets = check_existing_tweets(tweet_ids)

    # Filter out tweets that don't exist
    tweet_ids = [tweet_id for tweet_id in tweet_ids if tweet_id not in existing_tweet_ids]

    if not tweet_ids:
        print(f"‚úÖ Tweets already exist")
        sys.exit(0)

    # If not found, proceed with fetching the tweet
    tweets_data = await get_tweets(tweet_ids, flag)
    
    if not tweets_data:
        print(f"‚ùå Failed to fetch tweets")
        sys.exit(0)
    
    for tweet_data in tweets_data:
        # Create directory if it doesn't exist
        output_dir = f"tweets/{tweet_data['username']}"
        os.makedirs(output_dir, exist_ok=True)
        output_file = f"{output_dir}/input.json"
        
        # Save to file
        tweet_list = []
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                tweet_list = json.load(f)
        
        tweet_list.append(tweet_data)
        
        with open(output_file, 'w') as f:
            json.dump(tweet_list, f, indent=2)
            
        print(f"‚úÖ Tweet saved to {output_file}")
            
        # Add to database
        add_to_database(tweet_data)
        
        # Add to score
        add_to_score(tweet_data['username'], len(tweet_list), 0, 0, 1, ctxs)
        


if __name__ == "__main__":
    asyncio.run(main())
    print(f"‚úÖ Finished processing tweets")
